# ğŸ“š TÃ“M Táº®T CÃC THUáº¬T TOÃN ÄÃƒ Sá»¬ Dá»¤NG

## ğŸ¯ OVERVIEW

Project nÃ y sá»­ dá»¥ng **4 thuáº­t toÃ¡n Machine Learning** vÃ  **1 phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ outliers** chÃ­nh.

---

## 1ï¸âƒ£ TIá»€N Xá»¬ LÃ Dá»® LIá»†U

### IQR Method (Interquartile Range)
**Má»¥c Ä‘Ã­ch**: PhÃ¡t hiá»‡n vÃ  loáº¡i bá» outliers (giÃ¡ trá»‹ ngoáº¡i lai)

**CÃ´ng thá»©c**:
```
Q1 = Quartile thá»© 1 (25%)
Q3 = Quartile thá»© 3 (75%)
IQR = Q3 - Q1

Lower = Q1 - 1.5 Ã— IQR
Upper = Q3 + 1.5 Ã— IQR

Outliers = values < Lower OR values > Upper
```

**Æ¯u Ä‘iá»ƒm**: 
- âœ… Robust, khÃ´ng cáº§n phÃ¢n phá»‘i chuáº©n
- âœ… Dá»… hiá»ƒu vÃ  implement

**Vá»‹ trÃ­**: `notebooks/01_data_preprocessing.ipynb` - Cell "Xá»­ LÃ½ GiÃ¡ Trá»‹ Ngoáº¡i Lai"

---

## 2ï¸âƒ£ THUáº¬T TOÃN Há»ŒC MÃY

### ğŸ¥‰ Linear Regression
**Loáº¡i**: Simple regression
**CÃ´ng thá»©c**: `y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™`

**Khi dÃ¹ng**: Baseline model, dá»¯ liá»‡u tuyáº¿n tÃ­nh
**Performance**: Tháº¥p nháº¥t nhÆ°ng nhanh vÃ  dá»… hiá»ƒu

---

### ğŸ¥ˆ Random Forest
**Loáº¡i**: Ensemble (Bagging)
**CÃ¡ch hoáº¡t Ä‘á»™ng**: 
1. Táº¡o nhiá»u decision trees tá»« random subsets
2. Má»—i tree vote
3. Káº¿t quáº£ = average cá»§a táº¥t cáº£ trees

**Hyperparameters**:
- `n_estimators=100`: Sá»‘ trees
- `max_depth=20`: Äá»™ sÃ¢u
- `min_samples_split=5`: Min Ä‘á»ƒ split

**Khi dÃ¹ng**: Dá»¯ liá»‡u phá»©c táº¡p, cáº§n robust model
**Performance**: Cao, Ã­t overfitting

---

### ğŸ¥‡ XGBoost
**Loáº¡i**: Ensemble (Gradient Boosting)
**CÃ¡ch hoáº¡t Ä‘á»™ng**:
1. Build trees tuáº§n tá»±
2. Má»—i tree sá»­a lá»—i cá»§a tree trÆ°á»›c
3. Gradient descent + Regularization

**Hyperparameters**:
- `n_estimators=100`: Sá»‘ boosting rounds
- `max_depth=7`: Äá»™ sÃ¢u (nhá» hÆ¡n RF)
- `learning_rate=0.1`: Tá»‘c Ä‘á»™ há»c

**Khi dÃ¹ng**: Cáº§n accuracy cao nháº¥t
**Performance**: Ráº¥t cao, thÆ°á»ng win competitions

---

### âš¡ LightGBM
**Loáº¡i**: Ensemble (Fast Gradient Boosting)
**Innovations**:
- GOSS: Sampling thÃ´ng minh
- EFB: Bundle features
- Leaf-wise growth (vs level-wise)

**Hyperparameters**: TÆ°Æ¡ng tá»± XGBoost

**Khi dÃ¹ng**: Large datasets, cáº§n speed
**Performance**: TÆ°Æ¡ng Ä‘Æ°Æ¡ng XGBoost nhÆ°ng nhanh hÆ¡n 2-10x

---

## 3ï¸âƒ£ PHÆ¯Æ NG PHÃP ÄÃNH GIÃ

### Train-Test Split
- **Training**: 80% - Model há»c tá»« data nÃ y
- **Testing**: 20% - ÄÃ¡nh giÃ¡ trÃªn unseen data

### K-Fold Cross-Validation
- Chia data thÃ nh K folds (K=5)
- Má»—i fold lÃ m validation 1 láº§n
- Average cá»§a K scores â†’ robust evaluation

### Grid Search
- Thá»­ táº¥t cáº£ combinations cá»§a hyperparameters
- Chá»n combination tá»‘t nháº¥t dá»±a trÃªn CV score

---

## 4ï¸âƒ£ METRICS (CHá»ˆ Sá» ÄÃNH GIÃ)

| Metric | CÃ´ng Thá»©c | Range | Ã NghÄ©a |
|--------|-----------|-------|---------|
| **MAE** | `(1/n)Î£\|y-Å·\|` | [0,âˆ) | Sai sá»‘ TB, VNÄ |
| **RMSE** | `âˆšMSE` | [0,âˆ) | Penalize outliers |
| **RÂ²** | `1-(SSres/SStot)` | (-âˆ,1] | % variance giáº£i thÃ­ch |
| **MAPE** | `(100/n)Î£\|(y-Å·)/y\|` | [0,âˆ) | Sai sá»‘ %, dá»… hiá»ƒu |

---

## 5ï¸âƒ£ SO SÃNH NHANH

| Model | Speed | Accuracy | Complexity | Overfitting Risk |
|-------|-------|----------|------------|------------------|
| Linear Reg | âš¡âš¡âš¡âš¡âš¡ | â­â­ | Low | Low |
| Random Forest | âš¡âš¡âš¡ | â­â­â­â­ | Medium | Medium |
| XGBoost | âš¡âš¡ | â­â­â­â­â­ | High | Medium-High |
| LightGBM | âš¡âš¡âš¡âš¡ | â­â­â­â­â­ | High | Medium-High |

---

## ğŸ“‚ Vá»Š TRÃ TRONG CODE

### Preprocessing
- **File**: `src/preprocessing.py`
- **Notebook**: `notebooks/01_data_preprocessing.ipynb`
- **Thuáº­t toÃ¡n**: IQR outlier detection, Label Encoding

### Model Training
- **File**: `src/model.py`
- **Notebook**: `notebooks/02_model_training.ipynb`
- **Thuáº­t toÃ¡n**: Linear Regression, Random Forest, XGBoost, LightGBM

---

## ğŸš€ WORKFLOW Tá»”NG QUÃT

```
DATA
  â†“
[1] Load & EDA
  â†“
[2] Clean (remove duplicates, handle missing)
  â†“
[3] Remove Outliers (IQR Method) â† THUáº¬T TOÃN 1
  â†“
[4] Encode Categorical (Label Encoding)
  â†“
[5] Train-Test Split (80/20)
  â†“
[6] Train 4 Models:
    â€¢ Linear Regression      â† THUáº¬T TOÃN 2
    â€¢ Random Forest          â† THUáº¬T TOÃN 3
    â€¢ XGBoost                â† THUáº¬T TOÃN 4
    â€¢ LightGBM               â† THUáº¬T TOÃN 5
  â†“
[7] Evaluate with Metrics (MAE, RMSE, RÂ², MAPE)
  â†“
[8] Cross-Validation
  â†“
[9] Select Best Model
  â†“
[10] Deploy (Save model)
```

---

## ğŸ“– TÃ€I LIá»†U CHI TIáº¾T

Xem file `GIAI_THICH_THUAT_TOAN.md` Ä‘á»ƒ cÃ³:
- âœ… Giáº£i thÃ­ch chi tiáº¿t tá»«ng thuáº­t toÃ¡n
- âœ… CÃ´ng thá»©c toÃ¡n há»c Ä‘áº§y Ä‘á»§
- âœ… VÃ­ dá»¥ cá»¥ thá»ƒ
- âœ… Æ¯u/nhÆ°á»£c Ä‘iá»ƒm
- âœ… Best practices
- âœ… Tips Ä‘á»ƒ trÃ¡nh overfitting

---

## âœ¨ KEY TAKEAWAYS

1. **IQR Method**: Loáº¡i bá» outliers robust, khÃ´ng cáº§n normal distribution
2. **Linear Regression**: Simple baseline, dá»… hiá»ƒu
3. **Random Forest**: Robust ensemble, Ã­t overfitting
4. **XGBoost**: Highest accuracy vá»›i proper tuning
5. **LightGBM**: Fastest training, tá»‘t cho big data
6. **Cross-Validation**: Essential Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ reliable
7. **Multiple Metrics**: DÃ¹ng nhiá»u metrics Ä‘á»ƒ hiá»ƒu model toÃ n diá»‡n

---

**ğŸ’¡ Tip**: Táº¥t cáº£ cÃ¡c cell trong notebooks Ä‘á»u cÃ³ comment chi tiáº¿t giáº£i thÃ­ch tá»«ng bÆ°á»›c!
